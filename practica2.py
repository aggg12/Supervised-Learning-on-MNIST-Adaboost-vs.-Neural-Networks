# -*- coding: utf-8 -*-
"""PRACTICA2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_2JSvR0Y2vcnRduTdJqxGorcfONogt9N
"""

#PRÁCTICA 2 SISTEMAS INTELIGENTES
#ALEJANDRO GARCÍA GIL 77042008N

import numpy as np
import keras
import time
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from keras.optimizers import Adam

#adaboost y decisionstump
class DecisionStump:
    def __init__(self, n_features):
        self.umbral = np.random.rand() * 255
        self.carac = np.random.randint(0, n_features)
        self.polaridad = np.random.choice([-1, 1])

    def predict(self, X):
        predictions = np.ones(X.shape[0])
        if self.polaridad == 1:
            predictions[X[:, self.carac] < self.umbral] = -1
        else:
            predictions[X[:, self.carac] >= self.umbral] = -1
        return predictions

class Adaboost:
    def __init__(self, T=50, A=100):
        self.T = T
        self.A = A
        self.clasificadores = []

    def fit(self, X, Y, verbose=False):
        n_observations, n_features = X.shape
        pesos = np.ones(n_observations) / n_observations
        for t in range(self.T):
            best_classifier = None
            errormin = float('inf')
            for a in range(self.A):
                classifier = DecisionStump(n_features)
                predictions = classifier.predict(X)
                error = np.sum(pesos * (predictions != Y))
                if error < errormin:
                    errormin = error
                    best_classifier = classifier
            alpha = 0.5 * np.log((1 - errormin) / max(errormin, 1e-10))
            predictions = best_classifier.predict(X)
            pesos *= np.exp(-alpha * Y * predictions)
            pesos /= np.sum(pesos)
            best_classifier.alpha = alpha
            self.clasificadores.append(best_classifier)

            if verbose:
                print(f"Añadido clasificador {t}: Carac={best_classifier.carac}, Umbral={best_classifier.umbral:.4f}, Polaridad={best_classifier.polaridad}, Error={errormin:.6f}")

    def predict(self, X, return_confidence=False):
        classifier_predictions = np.array([classifier.predict(X) for classifier in self.clasificadores])
        confidences = np.dot(classifier_predictions.T, np.array([classifier.alpha for classifier in self.clasificadores]))
        if return_confidence:
            return confidences
        return np.sign(confidences)

#entrenamiento del adaboost binario
def train_and_evaluate_adaboost_binary(digit, T, A, verbose=False):
    (X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()
    X_train = X_train.reshape(X_train.shape[0], -1)
    X_test = X_test.reshape(X_test.shape[0], -1)

    Y_train_binary = np.where(Y_train == digit, 1, -1)
    Y_test_binary = np.where(Y_test == digit, 1, -1)

    adaboost = Adaboost(T=T, A=A)
    start_time = time.time()
    adaboost.fit(X_train, Y_train_binary, verbose=verbose)
    end_time = time.time()

    train_predictions = adaboost.predict(X_train)
    train_accuracy = accuracy_score(Y_train_binary, train_predictions)

    test_predictions = adaboost.predict(X_test)
    test_accuracy = accuracy_score(Y_test_binary, test_predictions)

    print(f"Entrenando clasificador Adaboost para el dígito {digit}, T={T}, A={A}")
    print(f"Tasas acierto (train, test) y tiempo: {train_accuracy * 100:.2f}%, {test_accuracy * 100:.2f}%, {end_time-start_time:.3f} s.")

    return train_accuracy, test_accuracy, end_time-start_time

#mi adaboost
def train_and_evaluate_my_adaboost(digit, T, A, X_train, Y_train, X_test, Y_test, verbose=False):
    Y_train_binary = np.where(Y_train == digit, 1, -1)
    Y_test_binary = np.where(Y_test == digit, 1, -1)

    adaboost = Adaboost(T=T, A=A)
    start_time = time.time()
    adaboost.fit(X_train, Y_train_binary, verbose=verbose)
    end_time = time.time()

    test_predictions = adaboost.predict(X_test)
    test_accuracy = accuracy_score(Y_test_binary, test_predictions)

    if verbose:
      print(f"Entrenando clasificador manual para el dígito {digit}")

    return test_accuracy, end_time - start_time

#adaboost con scikit-learn
def train_and_evaluate_adaboost_sklearn(digit, T, A, X_train, Y_train, X_test, Y_test, verbose=False):
    Y_train_binary = np.where(Y_train == digit, 1, -1)
    Y_test_binary = np.where(Y_test == digit, 1, -1)

    ada_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=T)
    start_time = time.time()
    ada_clf.fit(X_train, Y_train_binary)
    end_time = time.time()

    y_pred = ada_clf.predict(X_test)
    accuracy = accuracy_score(Y_test_binary, y_pred)
    return accuracy, end_time - start_time

#clasificador adaboost
def evaluate_multiclass(T, A, verbose=False):
    (X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()
    X_train = X_train.reshape(X_train.shape[0], -1)
    X_test = X_test.reshape(X_test.shape[0], -1)

    # manual
    classifiers_manual = {}
    for digit in range(10):
        classifiers_manual[digit] = train_and_evaluate_my_adaboost(digit, T, A, X_train, Y_train, X_test, Y_test, verbose)

    # scikit-learn
    classifiers_sklearn = {}
    for digit in range(10):
        if verbose:
            print(f"Entrenando clasificador scikit-learn para el dígito {digit}")
        classifiers_sklearn[digit] = train_and_evaluate_adaboost_sklearn(digit, T, A, X_train, Y_train, X_test, Y_test, verbose)

    for digit in range(10):
        acc_manual, time_manual = classifiers_manual[digit]
        acc_sklearn, time_sklearn = classifiers_sklearn[digit]
        print(f"Dígito: {digit}, Acc Manual: {acc_manual:.2f}, Tiempo Manual: {time_manual:.2f}, Acc Sklearn: {acc_sklearn:.2f}, Tiempo Sklearn: {time_sklearn:.2f}")

def probar_multiclase():

    # Configuración comparación
    T = 5  # tocones implementación manual
    A = 20  # características implementación manual
    evaluate_multiclass(T, A, verbose=True)


#comparar versiones y optimizar clasificador
def plot_accuracy_and_time(T_values, A_values, results_manual, results_sklearn):

    accuracy_manual, time_manual, accuracy_sklearn, time_sklearn = [], [], [], []
    labels = []

    for T in T_values:
        for A in A_values:
            labels.append(f"T={T}, A={A}")
            accuracy_manual.append(results_manual[(T, A)][0])
            time_manual.append(results_manual[(T, A)][1])
            accuracy_sklearn.append(results_sklearn[(T, A)][0])
            time_sklearn.append(results_sklearn[(T, A)][1])


    plt.figure(figsize=(14, 6))
    plt.subplot(1, 2, 1)
    plt.plot(labels, accuracy_manual, label='Manual - Accuracy', marker='o')
    plt.plot(labels, accuracy_sklearn, label='Sklearn - Accuracy', marker='x')
    plt.xticks(rotation=45)
    plt.ylabel('Accuracy')
    plt.title('Comparación de Precisión')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(labels, time_manual, label='Manual - Time', marker='o')
    plt.plot(labels, time_sklearn, label='Sklearn - Time', marker='x')
    plt.xticks(rotation=45)
    plt.ylabel('Time (s)')
    plt.title('Comparación de Tiempo de Entrenamiento')
    plt.legend()

    plt.tight_layout()
    plt.show()


def compare_and_optimize_adaboost_with_graphs(T_values, A_values, digit):
    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()
    X_train = X_train.reshape(X_train.shape[0], -1)
    X_test = X_test.reshape(X_test.shape[0], -1)

    results_manual, results_sklearn = {}, {}
    for T in T_values:
        for A in A_values:
            # train_and_evaluate_my_adaboost
            accuracy_manual, time_manual = train_and_evaluate_my_adaboost(digit, T, A, X_train, Y_train, X_test, Y_test)
            results_manual[(T, A)] = (accuracy_manual, time_manual)

            # train_and_evaluate_adaboost_sklearn
            accuracy_sklearn, time_sklearn = train_and_evaluate_adaboost_sklearn(digit, T, A, X_train, Y_train, X_test, Y_test)
            results_sklearn[(T, A)] = (accuracy_sklearn, time_sklearn)

    plot_accuracy_and_time(T_values, A_values, results_manual, results_sklearn)

def graCompare():
  T_values = [10, 20, 40]
  A_values = [10, 20, 40]
  digit = 5  # Cambiar según el dígito a clasificar
  compare_and_optimize_adaboost_with_graphs(T_values, A_values, digit)


#compartiva de modelos
def comparar_clasificadores(resultados):
    """
    Compara el rendimiento de varios clasificadores multiclase implementados.

    Argumentos:
    resultados -- Un diccionario donde las claves son los nombres de los clasificadores y
                  los valores son diccionarios con métricas como 'precision', 'tiempo_entrenamiento', etc.

    Devuelve:
    Un resumen comparativo del rendimiento de los clasificadores.
    """
    #summary
    for clasificador, metricas in resultados.items():
        print(f"Clasificador: {clasificador}")
        for metrica, valor in metricas.items():
            print(f"  {metrica}: {valor}")
        print("\n")

    mejor_precision = max(resultados, key=lambda x: resultados[x]['precision'])
    mas_rapido = min(resultados, key=lambda x: resultados[x]['tiempo_entrenamiento'])

    print(f"El clasificador con mejor precisión es: {mejor_precision}")
    print(f"El clasificador más rápido es: {mas_rapido}")

# ej
resultados = {
    "Adaboost_1D": {"precision": 0.95, "tiempo_entrenamiento": 120},
    "Adaboost_2A": {"precision": 0.96, "tiempo_entrenamiento": 150},
    "MLP_2D": {"precision": 0.97, "tiempo_entrenamiento": 180},

}

comparar_clasificadores(resultados)
#T y A
def train_my_adaboost_binary(TA_values, digit, X_train, Y_train, X_test, Y_test):
    accuracies = []
    times = []

    for T, A in TA_values:
        Y_train_binary = np.where(Y_train == digit, 1, -1)
        Y_test_binary = np.where(Y_test == digit, 1, -1)
        start_time = time.time()
        ada_clf = AdaBoostClassifier(
            base_estimator=DecisionTreeClassifier(max_depth=1),
            n_estimators=T,
            learning_rate=1.0,
            algorithm='SAMME.R'
        )
        ada_clf.fit(X_train[:A], Y_train_binary[:A])

        # accuracy
        predictions = ada_clf.predict(X_test)
        accuracy = accuracy_score(Y_test_binary, predictions)
        accuracies.append(accuracy)

        # time
        end_time = time.time()
        times.append(end_time - start_time)

    T_values = [T for T, A in TA_values]
    fig, ax1 = plt.subplots()

    color = 'tab:red'
    ax1.set_xlabel('T (Number of Estimators)')
    ax1.set_ylabel('Accuracy (%)', color=color)
    ax1.plot(T_values, accuracies, color=color)
    ax1.tick_params(axis='y', labelcolor=color)

    ax2 = ax1.twinx()
    color = 'tab:blue'
    ax2.set_ylabel('Time (s)', color=color)
    ax2.plot(T_values, times, color=color)
    ax2.tick_params(axis='y', labelcolor=color)

    fig.tight_layout()
    plt.title('AdaBoost Performance for Digit Recognition')
    plt.show()

# t y a values
TA_values = [(10, 90), (12, 75), (14, 65), (15, 60), (18, 50), (20, 40), (21, 42), (22, 40), (24, 35), (26, 33), (27, 34), (30, 31)]

# MNIST
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
X_train = X_train.reshape(-1, 28*28)
X_test = X_test.reshape(-1, 28*28)
train_my_adaboost_binary(TA_values, 5, X_train, Y_train, X_test, Y_test)

def train_mlp_classifier(epochs=10, batch_size=200, learning_rate=0.001, hidden_layers=[512], activation='relu'):
    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()
    X_train = X_train.reshape(X_train.shape[0], -1).astype('float32') / 255
    X_test = X_test.reshape(X_test.shape[0], -1).astype('float32') / 255
    Y_train = to_categorical(Y_train)
    Y_test = to_categorical(Y_test)
    num_classes = Y_test.shape[1]

    model = Sequential()
    for i, neurons in enumerate(hidden_layers):
        if i == 0:
            model.add(Dense(neurons, input_dim=784, activation=activation))
        else:
            model.add(Dense(neurons, activation=activation))
    model.add(Dense(num_classes, activation='softmax'))

    adam = Adam(learning_rate=learning_rate)
    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])

    start_time = time.time()
    model.fit(X_train, Y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=2)
    training_time = time.time() - start_time

    scores = model.evaluate(X_test, Y_test, verbose=0)
    print(f"Configuración: {hidden_layers}, Learning Rate: {learning_rate}, Activation: {activation}")
    print(f"Error: {100-scores[1]*100:.2f}%, Tiempo de Entrenamiento: {training_time:.2f} segundos\n")

def probarMLP():
  train_mlp_classifier()
  train_mlp_classifier(hidden_layers=[512, 256], activation='sigmoid', learning_rate=0.01)


#graCompare() #compara scikit y manual y enseña la grafica para comparar
#probar_multiclase() #entrena los scikit y manual
probarMLP() #entrena la implementacion de MLP y muestra la exactitud
#train_and_evaluate_adaboost_binary(digit=9, T=20, A=10, verbose=True)
if __name__ == "__main__":
  #COMENTO PARA FACILITAR LA CORRECCIÓN
  #para comparar scikit y manual y, enseñar la gráfica:
  #graCompare()
  #para poder probar el clasificador multiclase:
  #probar_multiclase()
  #para entrenar y probar el clasificador mlp con mnist:
  probarMLP()
  #para entrenar y evaluar el adaboost binario:
  #train_and_evaluate_adaboost_binary(digit=9, T=20, A=10)